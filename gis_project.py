# -*- coding: utf-8 -*-
"""GIS_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ISruT9HJi1EWPqH7qa7Gmud492YDYz_
"""

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# Define your project root folder
root_dir = '/content/drive/MyDrive/EarthObs'

# Traverse the folder
for dirpath, dirnames, filenames in os.walk(root_dir):
    print(f'\nðŸ“‚ Directory: {dirpath}')

    # Show up to 10 files per directory
    for f in filenames[:10]:
        print(f'   ðŸ“„ {f}')

    # Indicate more files
    if len(filenames) > 10:
        print(f'   ...and {len(filenames) - 10} more files')

!pip install geopandas shapely pyproj matplotlib

import geopandas as gpd
import matplotlib.pyplot as plt
from shapely.geometry import box
import numpy as np
import folium
from shapely.geometry import Point

"""**Load shapefile and generate 60Ã—60 grid**"""

# Load Delhi-NCR shapefile
ncr_path = '/content/drive/MyDrive/EarthObs/delhi_ncr_region.geojson'
delhi_ncr = gpd.read_file(ncr_path)

# Reproject to EPSG:32643 (UTM Zone 43N)
delhi_ncr = delhi_ncr.to_crs(epsg=32643)

# Function to generate uniform grid
def create_grid(gdf, cell_size):
    minx, miny, maxx, maxy = gdf.total_bounds
    x_coords = np.arange(minx, maxx, cell_size)
    y_coords = np.arange(miny, maxy, cell_size)
    grid_cells = [box(x, y, x + cell_size, y + cell_size) for x in x_coords for y in y_coords]
    return gpd.GeoDataFrame({'geometry': grid_cells}, crs=gdf.crs)

# Generate 60x60 km grid
grid_60km = create_grid(delhi_ncr, 60000)

# Plot NCR + Grid
ax = delhi_ncr.plot(edgecolor='black', figsize=(15, 15))
grid_60km.boundary.plot(ax=ax, color='red', linewidth=1)
plt.title("Delhi-NCR Region with 60Ã—60 km Grid")
plt.show()

# Reproject to EPSG:4326 for folium compatibility
grid_wgs84 = grid_60km.to_crs(epsg=4326)
delhi_ncr_wgs84 = delhi_ncr.to_crs(epsg=4326)

# Get the center of Delhi-NCR
center = delhi_ncr_wgs84.geometry.unary_union.centroid
m = folium.Map(location=[center.y, center.x], zoom_start=8, tiles='CartoDB positron')

# Add grid polygons
for _, row in grid_wgs84.iterrows():
    sim_geo = row.geometry.simplify(0.001)
    geo_json = folium.GeoJson(data=sim_geo, style_function=lambda x: {
        'fillColor': 'transparent', 'color': 'red', 'weight': 1
    })
    geo_json.add_to(m)

# Add boundary for Delhi-NCR
for _, row in delhi_ncr_wgs84.iterrows():
    geo_json = folium.GeoJson(data=row.geometry, style_function=lambda x: {
        'fillColor': 'transparent', 'color': 'black', 'weight': 2
    })
    geo_json.add_to(m)
folium.LayerControl().add_to(m)
m

"""**Mark the four corners and the center of each grid cell**"""

# Add corners + center markers
for _, row in grid_wgs84.iterrows():
    bounds = row.geometry.bounds  # (minx, miny, maxx, maxy)
    minx, miny, maxx, maxy = bounds

    # Define corners
    corners = [
        (miny, minx),  # Bottom-left
        (miny, maxx),  # Bottom-right
        (maxy, minx),  # Top-left
        (maxy, maxx)   # Top-right
    ]

    # Center point
    center = ((miny + maxy) / 2, (minx + maxx) / 2)

    # Add corner markers
    for lat, lon in corners:
        folium.CircleMarker(location=[lat, lon], radius=2, color='blue', fill=True).add_to(m)

    # Add center marker
    folium.CircleMarker(location=center, radius=2, color='green', fill=True).add_to(m)

m

import os
import pandas as pd
rgb_path = '/content/drive/MyDrive/EarthObs/rgb'
image_files = os.listdir(rgb_path)

# Extract lat/lon from filenames like 28.2266_77.5234.png
def extract_latlon(filename):
    parts = filename.replace('.png', '').split('_')
    if len(parts) == 2:
        try:
            lat = float(parts[0])
            lon = float(parts[1])
            return lat, lon, filename
        except:
            return None
    return None

# Apply and clean
coords_data = [extract_latlon(f) for f in image_files]
coords_data = [x for x in coords_data if x is not None]

# Convert to GeoDataFrame
df_coords = pd.DataFrame(coords_data, columns=['lat', 'lon', 'filename'])
gdf_coords = gpd.GeoDataFrame(df_coords, geometry=gpd.points_from_xy(df_coords['lon'], df_coords['lat']))
gdf_coords.set_crs(epsg=4326, inplace=True)
gdf_coords_proj = gdf_coords.to_crs(epsg=32643)

# Spatial filter
filtered = gpd.sjoin(gdf_coords_proj, grid_60km, predicate='within', how='inner')

# Report
print("Images before filtering:", len(image_files))
print("Images after filtering: ", len(filtered))

!pip install rasterio

import rasterio
from rasterio.windows import Window
from tqdm.notebook import tqdm
import numpy as np

"""**Reproject filtered GeoDataFrame to EPSG:4326**"""

# Reproject to match raster's CRS
filtered_wgs84 = filtered.to_crs(epsg=4326)

raster_path = '/content/drive/MyDrive/EarthObs/worldcover_bbox_delhi_ncr_2021.tif'
patches = []
valid_indices = []

with rasterio.open(raster_path) as src:
    transform = src.transform
    width, height = src.width, src.height

    for idx, row in tqdm(filtered_wgs84.iterrows(), total=len(filtered_wgs84)):
        x, y = row.geometry.x, row.geometry.y

        # Convert to pixel indices
        row_pix, col_pix = ~transform * (x, y)
        row_pix, col_pix = int(row_pix), int(col_pix)

        window = Window(col_pix - 64, row_pix - 64, 128, 128)

        try:
            patch = src.read(1, window=window)
            if patch.shape == (128, 128):
                patches.append(patch)
                valid_indices.append(idx)
            else:
                patches.append(None)
        except:
            patches.append(None)

from scipy.stats import mode

labels = []
valid_filenames = []

for idx, patch in zip(valid_indices, patches):
    if patch is not None:
        # Compute the mode (most frequent land cover class)
        patch_mode = mode(patch.flatten(), keepdims=False).mode
        labels.append(int(patch_mode))
        valid_filenames.append(filtered.iloc[idx]['filename'])

print(f"Total patches extracted: {len(patches)}")
print(f"Valid patches (not None): {len([p for p in patches if p is not None])}")

"""**Compute mode-based labels**"""

from scipy.stats import mode

labels = []
valid_filenames = []

for idx, patch in zip(valid_indices, patches):
    patch_mode = mode(patch.flatten(), keepdims=False).mode
    labels.append(int(patch_mode))
    valid_filenames.append(filtered.iloc[idx]['filename'])

# Show first 5 filename â†’ class
for i in range(5):
    print(f"{valid_filenames[i]} â†’ class {labels[i]}")

"""**Map ESA Class Codes to 11 Standardized Labels**"""

# Define mapping
esa_class_map = {
    10: "Tree Cover",
    20: "Shrubland",
    30: "Grassland",
    40: "Cropland",
    50: "Built-up",
    60: "Bare/Sparse",
    70: "Snow/Ice",
    80: "Water",
    90: "Wetlands",
    95: "Mangroves",
    100: "Moss/Lichen"
}

# Create label names
label_names = [esa_class_map.get(code, "Unknown") for code in labels]

"""**Create DataFrame & split**"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Create final DataFrame
df = pd.DataFrame({
    'filename': valid_filenames,
    'class_code': labels,
    'class_name': label_names
})

# Train-test split (70% train, 30% test)
train_df, test_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['class_code'])

print("Train size:", len(train_df))
print("Test size:", len(test_df))

"""**Class distribution plots**"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 5))

# Train set distribution
plt.subplot(1, 2, 1)
sns.countplot(data=train_df, y='class_name', order=train_df['class_name'].value_counts().index)
plt.title("Train Set Class Distribution")
plt.xlabel("Count")
plt.ylabel("Class")

# Test set distribution
plt.subplot(1, 2, 2)
sns.countplot(data=test_df, y='class_name', order=test_df['class_name'].value_counts().index)
plt.title("Test Set Class Distribution")
plt.xlabel("Count")
plt.ylabel("Class")

plt.tight_layout()
plt.show()

"""**Install & Import PyTorch**"""

!pip install torch torchvision torchaudio

import os
from PIL import Image
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models
from tqdm import tqdm
from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import random

"""**Define label mapping and prepare datasets**"""

# Label encoding: class_name â†’ integer index
class_names = sorted(df['class_name'].unique())
label_to_index = {name: i for i, name in enumerate(class_names)}
index_to_label = {i: name for name, i in label_to_index.items()}

# Add numeric class index to DataFrames
train_df['label_idx'] = train_df['class_name'].map(label_to_index)
test_df['label_idx'] = test_df['class_name'].map(label_to_index)

"""**Define custom Dataset**"""

class LandCoverDataset(Dataset):
    def __init__(self, dataframe, root_dir, transform=None):
        self.df = dataframe.reset_index(drop=True)
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        img_path = os.path.join(self.root_dir, self.df.loc[idx, 'filename'])
        image = Image.open(img_path).convert('RGB')
        label = self.df.loc[idx, 'label_idx']
        if self.transform:
            image = self.transform(image)
        return image, label

"""**Set paths, transforms & dataloaders**"""

# Path to RGB image directory
rgb_dir = '/content/drive/MyDrive/EarthObs/rgb'

# Transforms
transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor()
])

# Datasets and loaders
train_dataset = LandCoverDataset(train_df, rgb_dir, transform)
test_dataset = LandCoverDataset(test_df, rgb_dir, transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)

"""**Model setup and training loop**"""

# Detect device (GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Load ResNet18 and modify final layer
num_classes = len(label_to_index)

model = models.resnet18(pretrained=True)
model.fc = nn.Linear(model.fc.in_features, num_classes)
model = model.to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005)

"""**Train the model, Epochs=5**"""

num_epochs = 5

for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    correct = 0

    for images, labels in tqdm(train_loader):
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        correct += (predicted == labels).sum().item()

    acc = correct / len(train_dataset)
    print(f"Epoch [{epoch+1}/{num_epochs}] - Loss: {running_loss:.4f} - Train Accuracy: {acc:.4f}")

"""**Predict on Test Set**"""

model.eval()
all_preds = []
all_labels = []

with torch.no_grad():
    for images, labels in tqdm(test_loader):
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, preds = torch.max(outputs, 1)

        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

from sklearn.metrics import classification_report, f1_score

# Macro F1 (equal weight per class)
macro_f1 = f1_score(all_labels, all_preds, average='macro')
print(f"Macro F1 Score (custom): {macro_f1:.4f}")

# Full classification report
print("\nClassification Report:")
print(classification_report(all_labels, all_preds, target_names=class_names))

"""**optional - F1 via torchmetrics**"""

!pip install torchmetrics
from torchmetrics.classification import MulticlassF1Score
import torchmetrics

# Convert to tensors
preds_tensor = torch.tensor(all_preds)
labels_tensor = torch.tensor(all_labels)

f1_metric = MulticlassF1Score(num_classes=len(class_names), average='macro')
torch_f1 = f1_metric(preds_tensor, labels_tensor)
print(f"ðŸ”¥ Macro F1 Score (torchmetrics): {torch_f1:.4f}")

"""**Confusion Matrix**"""

# Generate confusion matrix
cm = confusion_matrix(all_labels, all_preds)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

# Plot
plt.figure(figsize=(10, 8))
sns.heatmap(cm_normalized, annot=True, fmt=".2f", cmap="Blues",
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Normalized Confusion Matrix")
plt.tight_layout()
plt.show()

"""Key Insights:
Cropland and Built-up are predicted most accurately.

For example:

    ~69% of actual Cropland was correctly predicted as Cropland.
    ~72% of actual Built-up was correctly predicted.

- Minority classes like Water, Wetlands, and Bare/Sparse are poorly predicted.
- Most of these are wrongly classified as Built-up or Cropland.

Confusion is especially strong between:

 1. Shrubland vs Built-up
 2. Tree Cover vs Cropland
 3. Grassland vs Cropland

The matrix confirms the modelâ€™s bias toward dominant classes, which is a direct consequence of class imbalance in the dataset.

**Show sample predictions**
"""

# Map label index to name
def label_name(idx):
    return index_to_label[int(idx)]

# Zip results
predictions = list(zip(all_preds, all_labels, test_df['filename'].values))

# Separate correct and incorrect
correct = [p for p in predictions if p[0] == p[1]]
incorrect = [p for p in predictions if p[0] != p[1]]

# Function to plot images
def show_predictions(samples, title):
    plt.figure(figsize=(15, 3))
    for i, (pred, true, filename) in enumerate(random.sample(samples, min(5, len(samples)))):
        img_path = os.path.join(rgb_dir, filename)
        img = Image.open(img_path)

        plt.subplot(1, 5, i+1)
        plt.imshow(img)
        plt.title(f"Pred: {label_name(pred)}\nTrue: {label_name(true)}")
        plt.axis('off')
    plt.suptitle(title)
    plt.show()

# Show correct and incorrect predictions
show_predictions(correct, "Correct Predictions")
show_predictions(incorrect, "Incorrect Predictions")

"""Correct Predictions:

   Images clearly show distinct features:
   1. Urban patterns â†’ correctly classified as Built-up
   2. Farm textures â†’ correctly classified as Cropland
These patches belong to the dominant, well-represented classes.

Incorrect Predictions:
Many images that are visually cropland are predicted as Built-up, and vice versa.

This reflects:
1. Mixed land-use regions where even human eyes would struggle.
2. Limitations of RGB-only data for subtle land cover classes.

The model performs reliably on dominant classes like Cropland and Built-up, but struggles with underrepresented and spectrally similar classes due to dataset imbalance and visual overlap. The confusion matrix and visual predictions confirm this pattern and highlight the need for richer input data or better class balancing techniques
"""

